{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrajanNieto/cough-analyzer-Covid/blob/main/01_P2_CovidCough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Proyecto de clasificación 'Identificacion de personas con covid'\n",
        " ----\n",
        "  \n",
        "  University : UTEC \\\\\n",
        "  Course       : Machine Learning \\\\\n",
        "  Professor    : Cristian López Del Alamo \\\\\n",
        "  Topic        : SVM \\\\\n",
        "  Deadline      : 06-07-2025\n",
        "   \n",
        "\n",
        " ----\n",
        "\n",
        "Write the names and surnames of the members and the percentage of participation of each one in the development of the practice:\n",
        " - Integrante 1: Lopez Medina Sebastian 100%\n",
        " - Integrante 2: Nieto Espinoza Brajan Esteban 100%\n",
        " - Integrante 3: Tapia Chasquibol Mateo 100%\n",
        "\n",
        " ----\n",
        " The objective of this project is to classify patients as either having COVID or not, using only the sound of the patient’s cough. For this, your group can use libraries to obtain the best feature vector to represent the sound of the\n",
        "cough."
      ],
      "metadata": {
        "id": "WrEdLIAuVClr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1  Librerias y datos\n",
        "# ============================================\n",
        "# -------------------------------------------\n",
        "# 1.1 Clonar repositorio\n",
        "# -------------------------------------------\n",
        "!git clone https://github.com/BrajanNieto/Mlearning.git\n",
        "%cd Mlearning\n",
        "# -------------------------------------------\n",
        "# 1.2 Librerías\n",
        "# -------------------------------------------\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "#  Librerías extras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cvxopt\n",
        "import librosa\n",
        "import pywt\n",
        "from scipy.signal import butter, filtfilt\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.utils import resample\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "mXOi3TDd27cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d361a80d-ce58-4a87-e8df-bb2c4688dc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mlearning'...\n",
            "remote: Enumerating objects: 1551, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 1551 (delta 1), reused 0 (delta 0), pack-reused 1546 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1551/1551), 417.26 MiB | 26.01 MiB/s, done.\n",
            "Resolving deltas: 100% (265/265), done.\n",
            "Updating files: 100% (1435/1435), done.\n",
            "/content/Mlearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Preprocessing y Extracción de Features\n",
        "\n",
        "**Puntos clave a tener en cuenta:**\n",
        "\n",
        "1. **`normalize_audio(audio)`**  \n",
        "   - Escala el pico máximo de la señal a 0.99.  \n",
        "   - Añade un pequeño epsilon (`1e-6`) para evitar división por cero.  \n",
        "\n",
        "2. **`remove_silence(audio, sr, top_db=50)`**  \n",
        "   - Usa `librosa.effects.split` para identificar segmentos con energía por encima de `top_db`.  \n",
        "   - Reconstruye el audio concatenando solo los intervalos activos (sin silencio).\n",
        "\n",
        "3. **`segment_audio(audio, sr, duration=1.5, hop=0.75)`**  \n",
        "   - Crea fragmentos superpuestos de longitud fija (`duration` segundos) con un salto (`hop`) entre inicios.  \n",
        "   - Calcula `frame_length = duration * sr` y `hop_length = hop * sr`.  \n",
        "   - Genera una lista de segmentos para procesamiento batch.\n",
        "\n",
        "4. **`extract_features(segment, sr)`**  \n",
        "   - Calcula **128 MFCCs** con `n_fft=512` y `hop_length=256`, y luego obtiene su valor medio.  \n",
        "   - Calcula la **tasa de cruce por cero** (zero-crossing rate) y toma su media.  \n",
        "   - Concatena ambos para un vector final de características.\n",
        "\n",
        "5. **Observación sobre dimensiones**  \n",
        "   - Aunque el docstring menciona “vector de 125 dimensiones”, la concatenación de 128 MFCCs más 1 valor de ZCR da **129 dimensiones**. Comprueba que downstream acepte este tamaño."
      ],
      "metadata": {
        "id": "aoKT_uKHBXc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 2  Funciones para  de Procesamiento de Audio\n",
        "# ============================================\n",
        "\n",
        "def normalize_audio(audio):\n",
        "    \"\"\"Normaliza el pico máximo a 0.99.\"\"\"\n",
        "    peak = np.max(np.abs(audio)) + 1e-6\n",
        "    return audio / peak * 0.99\n",
        "\n",
        "def remove_silence(audio, sr, top_db=50):\n",
        "    \"\"\"Recorta silencios basados en un umbral de decibelios.\"\"\"\n",
        "    intervals = librosa.effects.split(audio, top_db=top_db)\n",
        "    return np.concatenate([audio[s:e] for s, e in intervals])\n",
        "\n",
        "def segment_audio(audio, sr, duration=1.5, hop=0.75):\n",
        "    \"\"\"Divide en fragmentos superpuestos de duración fija.\"\"\"\n",
        "    fl = int(duration * sr)\n",
        "    hl = int(hop * sr)\n",
        "    return [audio[i:i+fl] for i in range(0, len(audio)-fl, hl)]\n",
        "\n",
        "def extract_features(segment, sr):\n",
        "    \"\"\"Calcula MFCCs (128) y zero‐crossing rate (1) y devuelve vector de 125 dims.\"\"\"\n",
        "    mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=128, n_fft=512, hop_length=256)\n",
        "    zcr   = librosa.feature.zero_crossing_rate(segment)\n",
        "    return np.hstack([mfccs.mean(axis=1), zcr.mean()])"
      ],
      "metadata": {
        "id": "S2xbTfhRz7U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación de DataFrame de Features\n",
        "\n",
        "- Recorre carpetas `Negative`, `Positive` y `Unknown`, cargando sólo `.wav`.  \n",
        "- Normaliza audio y elimina silencios antes de segmentar.  \n",
        "- Segmenta en ventanas superpuestas, extrae 128 MFCCs + ZCR por segmento.  \n",
        "- Genera filas `[filename, label, mfcc…, zcr]`, filtra `Unknown`, no se tomara en cuenta como una categoria adicional y mapea tiquetas a 0/1.  \n"
      ],
      "metadata": {
        "id": "KBJVtPo6Bemf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 3  Generación de registros y DataFrame\n",
        "# ============================================\n",
        "base_path = 'cleaned_data'\n",
        "records = []\n",
        "for label in ['Negative', 'Positive', 'Unknown']:\n",
        "    folder = os.path.join(base_path, label)\n",
        "    for fname in os.listdir(folder):\n",
        "        if not fname.lower().endswith('.wav'):\n",
        "            continue\n",
        "        path = os.path.join(folder, fname)\n",
        "        audio, sr = librosa.load(path, sr=16000)\n",
        "        # 3.1. Preprocesamiento\n",
        "        audio = normalize_audio(audio)\n",
        "        audio = remove_silence(audio, sr)\n",
        "        # 3.2. Segmentación\n",
        "        segments = segment_audio(audio, sr)\n",
        "        # 3.3. Extracción de features de cada segmento\n",
        "        for seg in segments:\n",
        "            feats = extract_features(seg, sr)\n",
        "            records.append([fname, label] + feats.tolist())\n",
        "\n",
        "# 3.4. Crear DataFrame con 'filename' y 'label'\n",
        "cols = ['filename', 'label'] + [f'mfcc_{i}' for i in range(128)] + ['zcr']\n",
        "df = pd.DataFrame(records, columns=cols)\n",
        "\n",
        "#3.5\n",
        "df = df[df['label'] != 'Unknown'].copy()\n",
        "mapping = {'Negative': 0, 'Positive': 1}\n",
        "df['label'] = df['label'].map(mapping)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPys7xlp17LH",
        "outputId": "d3bdc295-0ca1-4123-a2de-16d1725fec26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     filename  label      mfcc_0     mfcc_1     mfcc_2  \\\n",
            "0  602_Negative_female_24.wav      0 -372.318604  57.342354 -45.061600   \n",
            "1    866_Negative_male_29.wav      0 -357.252380  47.288013 -13.204280   \n",
            "2    866_Negative_male_29.wav      0 -364.362854  42.072475 -17.226746   \n",
            "3    866_Negative_male_29.wav      0 -385.721832  33.958363 -14.124773   \n",
            "4    866_Negative_male_29.wav      0 -389.195892  31.053137 -16.161470   \n",
            "\n",
            "      mfcc_3     mfcc_4     mfcc_5     mfcc_6     mfcc_7  ...  mfcc_119  \\\n",
            "0 -17.433853 -44.516247  -0.497882 -16.115273 -24.031391  ... -0.509453   \n",
            "1  12.651052 -27.716703   3.609981 -35.844666 -12.021462  ... -0.691036   \n",
            "2  12.977987 -28.532749   8.233962 -43.290474  -8.629445  ... -0.800812   \n",
            "3  26.623161 -12.464399  15.497139 -44.528969  -5.281589  ... -0.531144   \n",
            "4  21.019720 -12.703656  11.125703 -44.778397  -5.335186  ... -0.324802   \n",
            "\n",
            "   mfcc_120  mfcc_121  mfcc_122  mfcc_123  mfcc_124  mfcc_125  mfcc_126  \\\n",
            "0 -0.365180 -0.550728 -0.165455 -0.483451 -1.355135 -1.080396 -0.716505   \n",
            "1 -0.888819 -0.120295 -0.532270 -0.711712 -1.338131 -1.544193 -2.141667   \n",
            "2 -0.641932 -0.169620 -0.332620 -0.405354 -1.219086 -1.453555 -2.041320   \n",
            "3 -0.607188 -0.608565 -0.720199 -0.775048 -1.053042 -1.449840 -1.455495   \n",
            "4 -0.731272 -0.537169 -0.758031 -0.830815 -1.276065 -1.627204 -1.266470   \n",
            "\n",
            "   mfcc_127       zcr  \n",
            "0 -1.105229  0.248951  \n",
            "1 -1.583054  0.289062  \n",
            "2 -1.719377  0.308822  \n",
            "3 -1.695817  0.324240  \n",
            "4 -1.420950  0.326494  \n",
            "\n",
            "[5 rows x 131 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split de Datos por Archivo\n",
        "\n",
        "- Usa `GroupShuffleSplit` para dividir el 80% train y 20% test garantizando que segmentos del mismo archivo no se mezclen y que no haya overfiting.  \n",
        "- Genera `train_df` y `test_df` con índices reestablecidos.  \n",
        "- Añade columna `sex`: si el nombre contiene “female” (no sensible a mayúsculas) → 1; si no → 0.  \n",
        "- Elimina la columna `filename` al final.  \n"
      ],
      "metadata": {
        "id": "i9ZD8CwNELN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# 4  Split a nivel de archivo\n",
        "# ============================================\n",
        "\n",
        "# 4.1 Creamos un splitter que separa por grupos (filename)\n",
        "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(\n",
        "    splitter.split(df, y=df['label'], groups=df['filename'])\n",
        ")\n",
        "\n",
        "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
        "# 4.2\n",
        "for subset in (train_df, test_df):\n",
        "    # 1 = female, 0 = male (u otros)\n",
        "    subset['sex'] = np.where(\n",
        "        subset['filename'].str.contains('female', case=False),\n",
        "        1, 0\n",
        "    )\n",
        "    subset.drop(columns=['filename'], inplace=True)\n"
      ],
      "metadata": {
        "id": "JJRvHnUBC0XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de Conjuntos de Entrenamiento y Prueba\n",
        "\n",
        "- Separa **features** (`X`) y **labels** (`y`) en `train_df` y `test_df`.  \n",
        "- Elimina la columna `label` de `X_train` y `X_test`.  \n",
        "- Muestra dimensiones de cada set y conteo de clases (`value_counts`).  \n"
      ],
      "metadata": {
        "id": "On_Y4JmbE7Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 5  Preparar X e y para entrenamiento y prueba\n",
        "# ============================================\n",
        "X_train = train_df.drop(columns='label')\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_test  = test_df.drop(columns='label')\n",
        "y_test  = test_df['label']\n",
        "\n",
        "# 7. Vista previa (opcional)\n",
        "print(\"Train set:\", X_train.shape, y_train.value_counts().to_dict())\n",
        "print(\"Test  set:\", X_test.shape,  y_test.value_counts().to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-BYeichHJ2q",
        "outputId": "5b1dbb7e-8451-4743-e3d3-aea371590bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (3969, 130) {0: 3673, 1: 296}\n",
            "Test  set: (929, 130) {0: 850, 1: 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de Modelos y Búsqueda de Hiperparámetros\n",
        "\n",
        "- **Pipelines**  \n",
        "  - Escalado con `StandardScaler` para regresión logística, SVM y KNN; árbol de decisión sin escalado.  \n",
        "  - Todos los clasificadores usan `class_weight='balanced'` salvo KNN.\n",
        "\n",
        "- **Modelos incluidos**  \n",
        "  - **LogReg**: regresión logística (solver lbfgs, iteraciones 1000)  \n",
        "  - **SVM**: máquina de vectores de soporte con núcleo RBF (probabilidad desactivada)  \n",
        "  - **DT**: árbol de decisión  \n",
        "  - **KNN**: k vecinos más cercanos\n",
        "\n",
        "- **Grillas de hiperparámetros**  \n",
        "  - **LogReg**: búsqueda de constantes de regularización `C` (valores de 0.01 a 100)  \n",
        "  - **SVM**: `C` (0.1–10) y `gamma` (‘scale’ o 0.1)  \n",
        "  - **DT**: profundidades de 2 a 100 (o ilimitada) y muestras mínimas por hoja 1–10  \n",
        "  - **KNN**: número de vecinos entre 2 y 30\n"
      ],
      "metadata": {
        "id": "zSVmwq2tE9o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# 6. Definición de pipelines y grids\n",
        "# -------------------------------------------\n",
        "base_pipes = {\n",
        "    'LogReg': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', LogisticRegression(max_iter=1_000, solver='lbfgs',\n",
        "                                   class_weight='balanced', random_state=42))\n",
        "    ]),\n",
        "    'SVM': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', SVC(kernel='rbf', class_weight='balanced', probability=False,\n",
        "                    random_state=42))\n",
        "    ]),\n",
        "    'DT': Pipeline([\n",
        "        ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n",
        "    ]),\n",
        "    'KNN': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', KNeighborsClassifier())   # KNN no usa class_weight\n",
        "    ])\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'LogReg': {'clf__C': [0.01, 0.1, 1, 10, 100, 0.5, 0.6, 0.7]},\n",
        "    'SVM'   : {'clf__C': [0.1, 1, 1.5, 10],\n",
        "               'clf__gamma': ['scale', 0.1]},\n",
        "    'DT'    : {'clf__max_depth': [None, 5, 10],\n",
        "               'clf__min_samples_leaf': [2, 3, 5]},\n",
        "    'KNN'   : {'clf__n_neighbors': [2, 3, 4, 10, 20, 30]}\n",
        "}"
      ],
      "metadata": {
        "id": "70Qzbitxl2Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación con Grid Search y Validación Cruzada\n",
        "\n",
        "- Itera sobre cada pipeline en `base_pipes`.  \n",
        "- Usa `StratifiedKFold(n_splits=10, shuffle=True)` como CV interno.  \n",
        "- Configura `GridSearchCV` con:\n",
        "  - `scoring='f1_weighted'`  \n",
        "  - `n_jobs=-1` para paralelizar  \n",
        "  - `refit=True` para conservar el mejor modelo.  \n",
        "- Ajusta (`fit`) en `X_train`, `y_train` y almacena el mejor estimador en `best_estimators[name]`.  \n",
        "- Imprime el mejor `f1` obtenido y los parámetros óptimos (`best_params_`).  \n"
      ],
      "metadata": {
        "id": "MLUDWZFyFN-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# -------------------------------------------\n",
        "# 7. Grid Search + Bootstrap\n",
        "# -------------------------------------------\n",
        "B = 10  # número de remuestreos bootstrap\n",
        "best_estimators = {}\n",
        "\n",
        "for name, pipe in base_pipes.items():\n",
        "    print(f\"\\nModelo: {name}\")\n",
        "    print(\"-\" * 80)\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "\n",
        "    # Recorremos cada combinación de hiperparámetros\n",
        "    for params in ParameterGrid(param_grids[name]):\n",
        "        pipe.set_params(**params)\n",
        "        boot_metrics = []\n",
        "\n",
        "        # Generamos B muestras bootstrap\n",
        "        for b in range(B):\n",
        "            # Bootstrap sobre X_train, y_train\n",
        "            X_bs, y_bs = resample(\n",
        "                X_train, y_train,\n",
        "                replace=True,\n",
        "                random_state=42 + b\n",
        "            )\n",
        "            # Índices OOB (no seleccionados en bootstrap)\n",
        "            oob_mask = ~X_train.index.isin(X_bs.index)\n",
        "            X_oob, y_oob = X_train[oob_mask], y_train[oob_mask]\n",
        "            if y_oob.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # Entrenamos y predecimos\n",
        "            pipe.fit(X_bs, y_bs)\n",
        "            y_pred = pipe.predict(X_oob)\n",
        "\n",
        "            # Calculamos métricas weighted\n",
        "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "                y_oob, y_pred,\n",
        "                average='weighted',\n",
        "                zero_division=0\n",
        "            )\n",
        "            boot_metrics.append((prec, rec, f1))\n",
        "\n",
        "        # Si no hubo OOB válido, saltamos\n",
        "        if not boot_metrics:\n",
        "            continue\n",
        "\n",
        "        # Media y desviación de cada métrica\n",
        "        arr = np.array(boot_metrics)\n",
        "        mean_prec, mean_rec, mean_f1 = arr.mean(axis=0)\n",
        "        std_prec, std_rec, std_f1 = arr.std(axis=0)\n",
        "\n",
        "        # Imprimimos resultados para esta combinación\n",
        "        print(f\"params = {params}\")\n",
        "        print(\n",
        "            f\"  f1   = {mean_f1:.3f} ±{std_f1:.3f}  |  \"\n",
        "            f\"prec = {mean_prec:.3f} ±{std_prec:.3f}  |  \"\n",
        "            f\"rec  = {mean_rec:.3f} ±{std_rec:.3f}\"\n",
        "        )\n",
        "\n",
        "        # Actualizamos mejor modelo según f1 medio\n",
        "        if mean_f1 > best_score:\n",
        "            best_score = mean_f1\n",
        "            best_params = params\n",
        "            # Clonamos el pipe con esos parámetros\n",
        "            from sklearn.base import clone\n",
        "            best_estimators[name] = clone(pipe)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(\n",
        "        f\"✓ Mejor f1 medio = {best_score:.3f}  con params = {best_params}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "xLmFz_uGlv8i",
        "outputId": "d46a0868-2513-4c9b-8e7b-dbd2122add0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modelo: LogReg\n",
            "--------------------------------------------------------------------------------\n",
            "params = {'clf__C': 0.01}\n",
            "  f1   = 0.839 ±0.009  |  prec = 0.913 ±0.006  |  rec  = 0.797 ±0.013\n",
            "params = {'clf__C': 0.1}\n",
            "  f1   = 0.839 ±0.008  |  prec = 0.911 ±0.007  |  rec  = 0.798 ±0.012\n",
            "params = {'clf__C': 1}\n",
            "  f1   = 0.839 ±0.007  |  prec = 0.909 ±0.008  |  rec  = 0.797 ±0.010\n",
            "params = {'clf__C': 10}\n",
            "  f1   = 0.839 ±0.007  |  prec = 0.909 ±0.008  |  rec  = 0.798 ±0.010\n",
            "params = {'clf__C': 100}\n",
            "  f1   = 0.839 ±0.007  |  prec = 0.909 ±0.008  |  rec  = 0.798 ±0.010\n",
            "params = {'clf__C': 0.5}\n",
            "  f1   = 0.839 ±0.008  |  prec = 0.910 ±0.008  |  rec  = 0.797 ±0.011\n",
            "params = {'clf__C': 0.6}\n",
            "  f1   = 0.839 ±0.007  |  prec = 0.909 ±0.008  |  rec  = 0.798 ±0.011\n",
            "params = {'clf__C': 0.7}\n",
            "  f1   = 0.839 ±0.007  |  prec = 0.909 ±0.008  |  rec  = 0.798 ±0.010\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Mejor f1 medio = 0.839  con params = {'clf__C': 0.01}\n",
            "\n",
            "Modelo: SVM\n",
            "--------------------------------------------------------------------------------\n",
            "params = {'clf__C': 0.1, 'clf__gamma': 'scale'}\n",
            "  f1   = 0.902 ±0.009  |  prec = 0.925 ±0.007  |  rec  = 0.888 ±0.015\n",
            "params = {'clf__C': 0.1, 'clf__gamma': 0.1}\n",
            "  f1   = 0.889 ±0.006  |  prec = 0.855 ±0.007  |  rec  = 0.925 ±0.004\n",
            "params = {'clf__C': 1, 'clf__gamma': 'scale'}\n",
            "  f1   = 0.964 ±0.008  |  prec = 0.966 ±0.007  |  rec  = 0.967 ±0.007\n",
            "params = {'clf__C': 1, 'clf__gamma': 0.1}\n",
            "  f1   = 0.889 ±0.006  |  prec = 0.855 ±0.007  |  rec  = 0.925 ±0.004\n",
            "params = {'clf__C': 1.5, 'clf__gamma': 'scale'}\n",
            "  f1   = 0.963 ±0.008  |  prec = 0.966 ±0.007  |  rec  = 0.967 ±0.007\n",
            "params = {'clf__C': 1.5, 'clf__gamma': 0.1}\n",
            "  f1   = 0.889 ±0.006  |  prec = 0.855 ±0.007  |  rec  = 0.925 ±0.004\n",
            "params = {'clf__C': 10, 'clf__gamma': 'scale'}\n",
            "  f1   = 0.962 ±0.008  |  prec = 0.966 ±0.007  |  rec  = 0.966 ±0.007\n",
            "params = {'clf__C': 10, 'clf__gamma': 0.1}\n",
            "  f1   = 0.889 ±0.006  |  prec = 0.855 ±0.007  |  rec  = 0.925 ±0.004\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Mejor f1 medio = 0.964  con params = {'clf__C': 1, 'clf__gamma': 'scale'}\n",
            "\n",
            "Modelo: DT\n",
            "--------------------------------------------------------------------------------\n",
            "params = {'clf__max_depth': None, 'clf__min_samples_leaf': 2}\n",
            "  f1   = 0.883 ±0.008  |  prec = 0.886 ±0.008  |  rec  = 0.881 ±0.010\n",
            "params = {'clf__max_depth': None, 'clf__min_samples_leaf': 3}\n",
            "  f1   = 0.881 ±0.006  |  prec = 0.887 ±0.007  |  rec  = 0.875 ±0.007\n",
            "params = {'clf__max_depth': None, 'clf__min_samples_leaf': 5}\n",
            "  f1   = 0.873 ±0.006  |  prec = 0.889 ±0.007  |  rec  = 0.860 ±0.007\n",
            "params = {'clf__max_depth': 5, 'clf__min_samples_leaf': 2}\n",
            "  f1   = 0.792 ±0.018  |  prec = 0.898 ±0.007  |  rec  = 0.732 ±0.028\n",
            "params = {'clf__max_depth': 5, 'clf__min_samples_leaf': 3}\n",
            "  f1   = 0.791 ±0.019  |  prec = 0.898 ±0.007  |  rec  = 0.731 ±0.028\n",
            "params = {'clf__max_depth': 5, 'clf__min_samples_leaf': 5}\n",
            "  f1   = 0.791 ±0.019  |  prec = 0.898 ±0.007  |  rec  = 0.730 ±0.028\n",
            "params = {'clf__max_depth': 10, 'clf__min_samples_leaf': 2}\n",
            "  f1   = 0.862 ±0.013  |  prec = 0.894 ±0.008  |  rec  = 0.838 ±0.021\n",
            "params = {'clf__max_depth': 10, 'clf__min_samples_leaf': 3}\n",
            "  f1   = 0.859 ±0.012  |  prec = 0.893 ±0.007  |  rec  = 0.834 ±0.019\n",
            "params = {'clf__max_depth': 10, 'clf__min_samples_leaf': 5}\n",
            "  f1   = 0.853 ±0.011  |  prec = 0.893 ±0.008  |  rec  = 0.825 ±0.018\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Mejor f1 medio = 0.883  con params = {'clf__max_depth': None, 'clf__min_samples_leaf': 2}\n",
            "\n",
            "Modelo: KNN\n",
            "--------------------------------------------------------------------------------\n",
            "params = {'clf__n_neighbors': 2}\n",
            "  f1   = 0.965 ±0.006  |  prec = 0.968 ±0.005  |  rec  = 0.968 ±0.005\n",
            "params = {'clf__n_neighbors': 3}\n",
            "  f1   = 0.963 ±0.005  |  prec = 0.963 ±0.005  |  rec  = 0.965 ±0.004\n",
            "params = {'clf__n_neighbors': 4}\n",
            "  f1   = 0.950 ±0.008  |  prec = 0.955 ±0.007  |  rec  = 0.957 ±0.006\n",
            "params = {'clf__n_neighbors': 10}\n",
            "  f1   = 0.924 ±0.009  |  prec = 0.937 ±0.007  |  rec  = 0.940 ±0.006\n",
            "params = {'clf__n_neighbors': 20}\n",
            "  f1   = 0.902 ±0.008  |  prec = 0.928 ±0.008  |  rec  = 0.930 ±0.005\n",
            "params = {'clf__n_neighbors': 30}\n",
            "  f1   = 0.895 ±0.007  |  prec = 0.919 ±0.027  |  rec  = 0.927 ±0.005\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Mejor f1 medio = 0.965  con params = {'clf__n_neighbors': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación en Conjunto de Prueba\n",
        "\n",
        "- Para cada modelo óptimo en `best_estimators`:  \n",
        "  - Predice etiquetas sobre `X_test`.  \n",
        "  - Calcula **precision**, **recall** y **f1 ponderado**.  \n",
        "- Almacena los resultados en una lista de diccionarios con claves `model`, `precision`, `recall`, `f1`.  \n",
        "- Convierte la lista en un DataFrame, lo indexa por `model` y redondea a 3 decimales.  \n",
        "- Imprime la tabla resultante en formato Markdown (`to_markdown()`).  \n"
      ],
      "metadata": {
        "id": "Igrk54idFgYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# 8. Evaluación en test con los mejores estimadores\n",
        "# -------------------------------------------\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results_test = []\n",
        "\n",
        "for name, est in best_estimators.items():\n",
        "    # 0) Ajustamos el pipeline completo sobre X_train, y_train\n",
        "    est.fit(X_train, y_train)\n",
        "\n",
        "    # 1) Predecimos sobre X_test\n",
        "    y_pred = est.predict(X_test)\n",
        "\n",
        "    # 2) Calculamos precision, recall y f1 ponderado\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall    = recall_score(   y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1        = f1_score(       y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    results_test.append({\n",
        "        'model':     name,\n",
        "        'precision': precision,\n",
        "        'recall':    recall,\n",
        "        'f1':        f1,\n",
        "    })\n",
        "\n",
        "df_test = pd.DataFrame(results_test).set_index('model').round(3)\n",
        "print(df_test.to_markdown())\n"
      ],
      "metadata": {
        "id": "U8QE7yInnzLb",
        "outputId": "493fc82c-7c19-487b-ed56-96fc7bc3fb32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| model   |   precision |   recall |    f1 |\n",
            "|:--------|------------:|---------:|------:|\n",
            "| LogReg  |       0.873 |    0.76  | 0.805 |\n",
            "| SVM     |       0.916 |    0.926 | 0.904 |\n",
            "| DT      |       0.866 |    0.87  | 0.868 |\n",
            "| KNN     |       0.909 |    0.92  | 0.89  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svmb = best_estimators['SVM']\n",
        "y_pred = svmb.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=svmb.classes_, columns=svmb.classes_)\n",
        "print(\"Matriz de confusión (SVM):\")\n",
        "print(cm_df)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy total (SVM): {acc:.3f}\")"
      ],
      "metadata": {
        "id": "V9jH9M7mE7ed",
        "outputId": "577e55be-569d-466f-fd00-4656927512ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de confusión (SVM):\n",
            "     0   1\n",
            "0  846   4\n",
            "1   65  14\n",
            "\n",
            "Accuracy total (SVM): 0.926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serializar y Guardar el Modelo SVM\n",
        "\n",
        "- Se extrae el mejor estimador SVM: `svm_model = best_estimators['SVM']`.  \n",
        "- Se usa `joblib.dump` para guardar el objeto en disco:  \n",
        "  ```python\n",
        "  joblib.dump(svm_model, 'svm_model.pkl')"
      ],
      "metadata": {
        "id": "wKwOpvS4FtE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "svm_model = best_estimators['SVM']\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkRyzA_KJJot",
        "outputId": "6a208c77-82ae-411d-b3be-8e1842fb78b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get update -qq\n",
        "#!apt-get install -y libportaudio2 portaudio19-dev\n",
        "#!pip install --upgrade --force-reinstall sounddevice"
      ],
      "metadata": {
        "id": "NUrk17MXLXQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**USANDO EL ALGORITMO ENTRENADO:**  \n",
        "Est a secci[on ofrece una interfaz de línea de comandos para predecir si un clip de tos es “Positive” o “Negative” para covid (etiqueta binaria) usando un modelo SVM previamente entrenado con características MFCC + ZCR.  \n",
        "\n",
        "**Flujo principal:**  \n",
        "1. **Parseo de argumentos**  \n",
        "   - `--file`: ruta del audio (WAV, MP3, etc.)  \n",
        "   - `--sex`: 0 (male/otro) o 1 (female)  \n",
        "   - `--model`: archivo `.pkl` del modelo SVM  \n",
        "\n",
        "2. **Carga del modelo**  \n",
        "   - Valida existencia y carga con `joblib.load`.  \n",
        "\n",
        "3. **Obtención del archivo de audio**  \n",
        "   - Si no se pasa `--file`, permite subir en Colab o pide ruta local.  \n",
        "\n",
        "7. **Salida de resultados**  \n",
        "   - Muestra nombre de archivo, recuentos por segmento y porcentaje de confianza.  \n",
        "   - Imprime mensaje final personalizado:  \n",
        "     - Si etiqueta = 0 → “NO TIENES COVID!”  \n",
        "     - Si etiqueta ≠ 0 → “SI TIENES COVID U.U”  \n"
      ],
      "metadata": {
        "id": "9XzY-oPLGBpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inferencia de polaridad ('Positive'/'Negative') de un clip de voz\n",
        "usando un modelo SVM entrenado con MFCC + ZCR.\n",
        "\n",
        "✅ VERSIÓN SIMPLIFICADA - SOLO SUBIDA DE ARCHIVOS\n",
        "-------------------------------------------------\n",
        "1. Subir archivo de audio WAV\n",
        "2. Extrae features con librosa\n",
        "3. Ejecuta predicción con modelo SVM\n",
        "\n",
        "Autor original: <tu nombre>\n",
        "Última revisión: 29‑jun‑2025\n",
        "\"\"\"\n",
        "\n",
        "# ============================ Imports =======================================\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "\n",
        "# ---------- Dependencias opcionales según entorno ----------\n",
        "COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import files  # type: ignore\n",
        "    COLAB = True\n",
        "except ImportError:\n",
        "    # Ejecución local\n",
        "    pass\n",
        "\n",
        "# ========================== 1. Pre‑procesamiento ============================\n",
        "\n",
        "def normalize_audio(audio: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Normaliza a ±0.99 para evitar clipping.\"\"\"\n",
        "    peak = np.max(np.abs(audio)) + 1e-6\n",
        "    return audio / peak * 0.99\n",
        "\n",
        "\n",
        "def remove_silence(audio: np.ndarray, sr: int, top_db: int = 60) -> np.ndarray:\n",
        "    \"\"\"Elimina silencios usando librosa.effects.split.\"\"\"\n",
        "    intervals = librosa.effects.split(audio, top_db=top_db)\n",
        "    return (\n",
        "        np.concatenate([audio[s:e] for s, e in intervals])\n",
        "        if intervals.size\n",
        "        else audio\n",
        "    )\n",
        "\n",
        "\n",
        "def segment_audio(audio: np.ndarray, sr: int, duration: float = 1.5, hop: float = 0.75) -> list[np.ndarray]:\n",
        "    \"\"\"Ventanas deslizantes de `duration` s cada `hop` s.\"\"\"\n",
        "    fl, hl = int(duration * sr), int(hop * sr)\n",
        "    return [audio[i : i + fl] for i in range(0, len(audio) - fl + 1, hl)]\n",
        "\n",
        "\n",
        "def extract_features(segment: np.ndarray, sr: int) -> np.ndarray:\n",
        "    \"\"\"MFCC (128) + ZCR (1) → vector 129‑d.\"\"\"\n",
        "    mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=128, n_fft=512, hop_length=256)\n",
        "    zcr = librosa.feature.zero_crossing_rate(segment)\n",
        "    return np.hstack([mfccs.mean(axis=1), zcr.mean()])\n",
        "\n",
        "# ============================== 2. Modelo ====================================\n",
        "\n",
        "def load_model(path: str):\n",
        "    \"\"\"Carga el modelo serializado con joblib/pickle.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        sys.exit(f\"⚠️  No existe el modelo: {path}\")\n",
        "    return load(path)\n",
        "\n",
        "# ========================== 3. Audio helpers =================================\n",
        "\n",
        "def upload_audio_colab() -> str:\n",
        "    \"\"\"Subir archivo de audio en Colab.\"\"\"\n",
        "    print(\"📁 Por favor, selecciona un archivo de audio WAV...\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        sys.exit(\"⚠️  No se subió ningún archivo.\")\n",
        "\n",
        "    fname = next(iter(uploaded))\n",
        "    if not fname.lower().endswith((\".wav\", \".mp3\", \".m4a\", \".flac\")):\n",
        "        print(\"⚠️ Formato no óptimo. Se recomienda WAV, pero se intentará procesar.\")\n",
        "\n",
        "    return fname\n",
        "\n",
        "def get_audio_file_local() -> str:\n",
        "    \"\"\"Obtener archivo de audio en ejecución local.\"\"\"\n",
        "    while True:\n",
        "        file_path = input(\"📁 Ruta del archivo de audio: \").strip()\n",
        "        if os.path.isfile(file_path):\n",
        "            return file_path\n",
        "        print(\"⚠️ Archivo no encontrado. Intenta de nuevo.\")\n",
        "\n",
        "# =================== 4. Procesamiento de audio ===================\n",
        "\n",
        "def process_audio_file(audio_path: str, sex: int) -> pd.DataFrame:\n",
        "    \"\"\"Procesa archivo de audio y extrae features.\"\"\"\n",
        "    print(f\"🔊 Cargando audio: {audio_path}\")\n",
        "\n",
        "    # Cargar audio con librosa\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_path, sr=16_000)\n",
        "        print(f\"📊 Audio cargado: {len(audio)} samples a {sr} Hz ({len(audio)/sr:.2f} segundos)\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ Error cargando audio: {e}\")\n",
        "\n",
        "    # Preprocessing\n",
        "    audio = normalize_audio(audio)\n",
        "    audio = remove_silence(audio, sr)\n",
        "\n",
        "    if len(audio) < sr * 1.5:  # Menos de 1.5 segundos\n",
        "        raise RuntimeError(\"⚠️ Audio muy corto después de quitar silencios\")\n",
        "\n",
        "    print(f\"🔧 Audio procesado: {len(audio)/sr:.2f} segundos después de quitar silencios\")\n",
        "\n",
        "    # Segmentación\n",
        "    segments = segment_audio(audio, sr)\n",
        "    if not segments:\n",
        "        raise RuntimeError(\"No se pudieron crear segmentos de audio\")\n",
        "\n",
        "    print(f\"📊 Creados {len(segments)} segmentos para análisis\")\n",
        "\n",
        "    # Extracción de features\n",
        "    features = []\n",
        "    for i, segment in enumerate(segments):\n",
        "        feat = extract_features(segment, sr)\n",
        "        features.append(feat)\n",
        "        if i < 3:  # Mostrar solo los primeros 3\n",
        "            print(f\"  Segmento {i+1}: MFCC promedio = {feat[:5].round(3)}\")\n",
        "\n",
        "    # Crear DataFrame\n",
        "    df = pd.DataFrame(features, columns=[f\"mfcc_{i}\" for i in range(128)] + [\"zcr\"])\n",
        "    df.insert(0, \"sex\", sex)\n",
        "    df.insert(0, \"filename\", os.path.basename(audio_path))\n",
        "\n",
        "    return df\n",
        "\n",
        "def majority_vote(model, df_features: pd.DataFrame) -> tuple[str, Counter]:\n",
        "    \"\"\"Voto mayoritario sobre segmentos.\"\"\"\n",
        "    try:\n",
        "        X = df_features[model.feature_names_in_]\n",
        "    except AttributeError:\n",
        "        X = df_features.drop(columns=[\"filename\", \"sex\"], errors=\"ignore\")\n",
        "\n",
        "    preds = model.predict(X)\n",
        "    tally = Counter(preds)\n",
        "    return tally.most_common(1)[0][0], tally\n",
        "\n",
        "# ====================== 5. CLI / argumentos =================================\n",
        "\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser(description=\"Inferencia sobre audio WAV\")\n",
        "    ap.add_argument(\"--file\", help=\"Ruta del archivo de audio\")\n",
        "    ap.add_argument(\"--sex\", choices=[\"0\", \"1\"], help=\"0 = male/otro, 1 = female\")\n",
        "    ap.add_argument(\"--model\", default=\"svm_model.pkl\", help=\"Archivo del modelo SVM\")\n",
        "    args, _ = ap.parse_known_args()\n",
        "    return args\n",
        "\n",
        "# ========================== 6. Main =========================================\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    print(\"🤖 Cargando modelo SVM...\")\n",
        "    model = load_model(args.model)\n",
        "    print(\"✅ Modelo cargado correctamente\")\n",
        "\n",
        "    # ---------- Obtener sexo ----------\n",
        "    if args.sex is None:\n",
        "        print(\"\\n👤 Información del paciente:\")\n",
        "        while True:\n",
        "            sex_input = input(\"Sexo (0 = masculino/otro, 1 = femenino): \").strip()\n",
        "            if sex_input in (\"0\", \"1\"):\n",
        "                args.sex = sex_input\n",
        "                break\n",
        "            print(\"⚠️ Por favor ingresa 0 ó 1\")\n",
        "\n",
        "    sex = int(args.sex)\n",
        "\n",
        "    # ---------- Obtener archivo de audio ----------\n",
        "    if args.file:\n",
        "        # Archivo especificado por parámetro\n",
        "        if not os.path.isfile(args.file):\n",
        "            sys.exit(f\"⚠️ Archivo no encontrado: {args.file}\")\n",
        "        audio_path = args.file\n",
        "    else:\n",
        "        # Obtener archivo según el entorno\n",
        "        if COLAB:\n",
        "            audio_path = upload_audio_colab()\n",
        "        else:\n",
        "            audio_path = get_audio_file_local()\n",
        "\n",
        "    # ---------- Procesar audio ----------\n",
        "    print(f\"\\n🔄 Procesando archivo: {audio_path}\")\n",
        "    df = process_audio_file(audio_path, sex)\n",
        "\n",
        "    # ---------- Predicción ----------\n",
        "    print(\"\\n🧠 Ejecutando predicción...\")\n",
        "    final_label, counts = majority_vote(model, df)\n",
        "\n",
        "    # ---------- Resultado ----------\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"📁 Archivo: {os.path.basename(audio_path)}\")\n",
        "    print(f\"📊 Predicciones por segmento: {dict(counts)}\")\n",
        "    print(f\"🎯 RESULTADO FINAL: {final_label}\")\n",
        "    if final_label == 0:\n",
        "      print(f\"🎯 RESULTADO FINAL: {final_label} – NO TIENES COVID!\")\n",
        "    else:\n",
        "        print(f\"🎯 RESULTADO FINAL: {final_label} – SI TIENES COVID U.U\")\n",
        "    print(f\"📈 Confianza: {counts[final_label]}/{sum(counts.values())} segmentos\")\n",
        "\n",
        "    # Mostrar porcentaje de confianza\n",
        "    confidence_pct = (counts[final_label] / sum(counts.values())) * 100\n",
        "    print(f\"💯 Porcentaje de segmentos: {confidence_pct:.1f}%\")\n",
        "\n",
        "    # Mostrar detalles por segmento si hay pocos\n",
        "    if len(counts) <= 5:\n",
        "        print(\"\\n📋 Detalle por segmento:\")\n",
        "        for label, count in counts.most_common():\n",
        "            pct = (count / sum(counts.values())) * 100\n",
        "            print(f\"  {label}: {count} segmentos ({pct:.1f}%)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n❌ Cancelado por el usuario\")\n",
        "        sys.exit(0)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 Error: {e}\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "id": "BHsGVRiw12eZ",
        "outputId": "095adaab-5ba0-43b0-af57-714fb103dd9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Cargando modelo SVM...\n",
            "✅ Modelo cargado correctamente\n",
            "\n",
            "👤 Información del paciente:\n",
            "Sexo (0 = masculino/otro, 1 = femenino): 0\n",
            "📁 Por favor, selecciona un archivo de audio WAV...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12f3d219-8d49-4e36-8ebf-4db9022e6f03\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-12f3d219-8d49-4e36-8ebf-4db9022e6f03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 179_Positive_male_25.wav to 179_Positive_male_25.wav\n",
            "\n",
            "🔄 Procesando archivo: 179_Positive_male_25.wav\n",
            "🔊 Cargando audio: 179_Positive_male_25.wav\n",
            "📊 Audio cargado: 98304 samples a 16000 Hz (6.14 segundos)\n",
            "🔧 Audio procesado: 5.86 segundos después de quitar silencios\n",
            "📊 Creados 6 segmentos para análisis\n",
            "  Segmento 1: MFCC promedio = [-459.693   89.929  -13.312   10.389   -9.594]\n",
            "  Segmento 2: MFCC promedio = [-483.239   91.918   -5.656    5.673   -8.652]\n",
            "  Segmento 3: MFCC promedio = [-527.178   99.269  -11.375   -2.381   -9.091]\n",
            "\n",
            "🧠 Ejecutando predicción...\n",
            "\n",
            "============================================================\n",
            "📁 Archivo: 179_Positive_male_25.wav\n",
            "📊 Predicciones por segmento: {np.int64(1): 6}\n",
            "🎯 RESULTADO FINAL: 1\n",
            "🎯 RESULTADO FINAL: 1 – SI TIENES COVID U.U\n",
            "📈 Confianza: 6/6 segmentos\n",
            "💯 Porcentaje de segmentos: 100.0%\n",
            "\n",
            "📋 Detalle por segmento:\n",
            "  1: 6 segmentos (100.0%)\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}